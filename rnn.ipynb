{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road to Quijote-like text: RNNs\n",
    "\n",
    "Our goal is to generate Quijote-like text by implementing and training models on the Quijote novel. The idea is to implement the models from scratch and write down every step that I go trough.\n",
    "\n",
    "## Loading and preparing the data\n",
    "\n",
    "The first step is to load and prepare the dataset. First, we load the whole Quijote into a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038397"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"el_quijote.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "len(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, our goal is to predict the next character given a preceding sequence of characters, or **context**. We will understand it better with the following example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Context) -> (Next Character):\n",
      "'DON QUIJ' -> 'O'\n",
      "'ON QUIJO' -> 'T'\n",
      "'N QUIJOT' -> 'E'\n",
      "' QUIJOTE' -> ' '\n",
      "'QUIJOTE ' -> 'D'\n",
      "'UIJOTE D' -> 'E'\n",
      "'IJOTE DE' -> ' '\n",
      "'JOTE DE ' -> 'L'\n",
      "'OTE DE L' -> 'A'\n",
      "'TE DE LA' -> ' '\n",
      "'E DE LA ' -> 'M'\n",
      "' DE LA M' -> 'A'\n"
     ]
    }
   ],
   "source": [
    "context_size = 8 # The number of characters that we want for context\n",
    "\n",
    "context = text[:context_size]\n",
    "\n",
    "print(\"(Context) -> (Next Character):\")\n",
    "\n",
    "for c in text[context_size:20]:\n",
    "    print(f\"'{context}' -> '{c}'\")\n",
    "    context = context[1:] + c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will train a model with a dataset composed by a set of context sequences with its corresponding next characters extracted from the Quijote. In our case, we are working at **character-level**, but there are other approaches that work with words or different tokens. In fact, state-of-the-art models use different tokenization schemes (for example, see the [ChatGPT tokenizer](https://platform.openai.com/tokenizer)).\n",
    "\n",
    "DL models don't know how to directly read raw characters, so we need to properly encode them into integers. We will assign a unique integer to each different character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRING -> INTEGERS -> BACK TO STRING\n",
      "DON QUIJOTE -> [27, 38, 37, 1, 40, 44, 32, 33, 38, 43, 28] -> DON QUIJOTE\n"
     ]
    }
   ],
   "source": [
    "# obtain the different characters present in the text\n",
    "characters = list(sorted(set(text)))\n",
    "\n",
    "ctoi = {c:i for i, c in enumerate(characters)} # dictionary that maps a given character into its respective integer\n",
    "itoc = {i:c for c, i in ctoi.items()}\n",
    "\n",
    "example = text[:11]\n",
    "translated_example = [ctoi[c] for c in example]\n",
    "\n",
    "print(f\"STRING -> INTEGERS -> BACK TO STRING\")\n",
    "print(f\"{example} -> {translated_example} -> {''.join(itoc[i] for i in translated_example)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 3\n",
    "\n",
    "def build_dataset(text):\n",
    "    X, Y = [], []\n",
    "    context = text[:context_size]\n",
    "\n",
    "    for c in text[context_size:]:\n",
    "        X.append(torch.tensor([ctoi[c] for c in context], dtype=torch.float32))\n",
    "        Y.append(torch.tensor(ctoi[c], dtype=torch.float32))\n",
    "        context = context[1:] + c\n",
    "    \n",
    "    X = torch.stack(X)\n",
    "    Y = torch.stack(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "n = int(0.8*len(text))\n",
    "\n",
    "Xtr, Ytr = build_dataset(text[:n])\n",
    "Xval, Yval = build_dataset(text[n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([830714, 3]), torch.Size([207677, 3]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Xval.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
