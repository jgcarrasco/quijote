{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road to Quijote-like text: RNNs\n",
    "\n",
    "Our goal is to generate Quijote-like text by implementing and training models on the Quijote novel. The idea is to implement the models from scratch and write down every step that I go trough.\n",
    "\n",
    "## Loading and preparing the data\n",
    "\n",
    "The first step is to load and prepare the dataset. First, we load the whole Quijote into a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038397"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"el_quijote.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "len(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, our goal is to predict the next character given a preceding sequence of characters, or **context**. We will understand it better with the following example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Context) -> (Next Character):\n",
      "'DON QUIJ' -> 'O'\n",
      "'ON QUIJO' -> 'T'\n",
      "'N QUIJOT' -> 'E'\n",
      "' QUIJOTE' -> ' '\n",
      "'QUIJOTE ' -> 'D'\n",
      "'UIJOTE D' -> 'E'\n",
      "'IJOTE DE' -> ' '\n",
      "'JOTE DE ' -> 'L'\n",
      "'OTE DE L' -> 'A'\n",
      "'TE DE LA' -> ' '\n",
      "'E DE LA ' -> 'M'\n",
      "' DE LA M' -> 'A'\n"
     ]
    }
   ],
   "source": [
    "context_size = 8 # The number of characters that we want for context\n",
    "\n",
    "context = text[:context_size]\n",
    "\n",
    "print(\"(Context) -> (Next Character):\")\n",
    "\n",
    "for c in text[context_size:20]:\n",
    "    print(f\"'{context}' -> '{c}'\")\n",
    "    context = context[1:] + c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will train a model with a dataset composed by a set of context sequences with its corresponding next characters extracted from the Quijote. In our case, we are working at **character-level**, but there are other approaches that work with words or different tokens. In fact, state-of-the-art models use different tokenization schemes (for example, see the [ChatGPT tokenizer](https://platform.openai.com/tokenizer)).\n",
    "\n",
    "DL models don't know how to directly read raw characters, so we need to properly encode them into integers. We will assign a unique integer to each different character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRING -> INTEGERS -> BACK TO STRING\n",
      "DON QUIJOTE -> [27, 38, 37, 1, 40, 44, 32, 33, 38, 43, 28] -> DON QUIJOTE\n"
     ]
    }
   ],
   "source": [
    "# obtain the different characters present in the text\n",
    "characters = list(sorted(set(text)))\n",
    "\n",
    "ctoi = {c:i for i, c in enumerate(characters)} # dictionary that maps a given character into its respective integer\n",
    "itoc = {i:c for c, i in ctoi.items()}\n",
    "\n",
    "example = text[:11]\n",
    "translated_example = [ctoi[c] for c in example]\n",
    "\n",
    "print(f\"STRING -> INTEGERS -> BACK TO STRING\")\n",
    "print(f\"{example} -> {translated_example} -> {''.join(itoc[i] for i in translated_example)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will build the training and validation datasets. We will take the full text, and split it into sequences of length `context_size` and its preceding character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr = torch.Size([830714, 3]), Ytr = torch.Size([830714]), Xval = torch.Size([207677, 3]), Yval = torch.Size([207677])\n"
     ]
    }
   ],
   "source": [
    "context_size = 3\n",
    "\n",
    "def build_dataset(text):\n",
    "    X, Y = [], []\n",
    "    context = text[:context_size]\n",
    "\n",
    "    for c in text[context_size:]:\n",
    "        X.append(torch.tensor([ctoi[c] for c in context], dtype=torch.float32))\n",
    "        Y.append(torch.tensor(ctoi[c], dtype=torch.float32))\n",
    "        context = context[1:] + c\n",
    "    \n",
    "    X = torch.stack(X)\n",
    "    Y = torch.stack(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "n = int(0.8*len(text))\n",
    "\n",
    "Xtr, Ytr = build_dataset(text[:n])\n",
    "Xval, Yval = build_dataset(text[n:])\n",
    "print(f\"Xtr = {Xtr.shape}, Ytr = {Ytr.shape}, Xval = {Xval.shape}, Yval = {Yval.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is time to build the RNN model! It will be composed by three key components.:\n",
    "\n",
    "1. We will create an [`Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer, whose task will be to map each different token into a given vector. Intuitively, mapping the tokens into a vector space allows the model to group tokens into different regions according to its similarity or dissimilarity. It can be simply thought as a lookup table that stores the (learned) embeddings for each character in the text.\n",
    "\n",
    "    More formally, if we have a batch of sequences $X$ of shape $(B, T)$, where $B$ is the batch dimension and $T$ is the number of tokens in each sequence (i.e. `context_length`), we will lookup the value for every token on every sequence, thus we will obtain a tensor of shape ($B, T, C)$, where $C$ is the size of the embedding vector (i.e. `num_embedding`).\n",
    "\n",
    "2. We will create an Elman RNN layer as shown in the [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html). Essentially, for every element in the input sequence, the layer will compute the following function:\n",
    "\n",
    "    $$h_t = \\tanh(x_tW_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})$$\n",
    "\n",
    "    where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$. The idea is to iteratively compute an associated output $h_t$ for each token $x_t, i \\in \\{1, 2, 3, ..., n_{embed}\\}$ by taking into account both the current token and the hidden state at the previous step. Intuitively, this enables the RNN to model past temporal dependencies or features.  \n",
    "\n",
    "3. Finally, after applying one or more layers, we project the feature vector to a tensor of shape $(B, n_{chars})$, containing the prediction of the model for each sequence in the batch, where `n_chars` is the total number of unique characters present in the text. We can interpret this values as the **unnormalized log probabilities, or logits, of the next character**. For the sake of example and assuming that we are working with a single example, the output of the model will be:\n",
    "\n",
    "    $$l = (l_0, l_1, ..., l_n)$$\n",
    "\n",
    "    where $l_0$ will be the unnormalized log probability of the next character in the sequence being the one associated with 0. Exponentiating the logits, we obtain the unnormalized probabilities:\n",
    "\n",
    "    $$p_i = e^{l_i}$$\n",
    "\n",
    "    Then, we properly normalize the probabilities so that they sum to one:\n",
    "\n",
    "    $$p_i = \\dfrac{e^{l_i}}{\\sum_j e^{l_j}}$$ \n",
    "\n",
    "    Essentially, we want to **maximize** the likelihood of the correct character being predicted by the model, i.e., if the next character is $y_i$, then we want to manipulate the weights of the model so that $p_{y_i}$ is **increased**. Maximizing the likelihood of our training distribution is **equivalent** to minimizing the **negative log-likelihood, or [cross entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)**. To sum up, our model has to output a vector with the same length as the number of different characters in the text, and we will interpret the ouput as the logits of the next possible characters.\n",
    "\n",
    "We will build every component by using `PyTorch`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([830714, 3]), torch.Size([207677, 3]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
